{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''If running directly, \n",
    "\n",
    "This script is intended to go through any json files produced by the MegaDetector, and also get the class name from the filename, \n",
    "and produce a single parquet file with all the info needed for later cropping\n",
    "\n",
    "It could be improved by re-using this parquet the next time, and only processing new .json\n",
    "\n",
    "The output columns will be: ['File_Path', 'Mega_Class', 'Confidence', 'x_min', 'y_min', 'Width', 'Height', 'Species', 'Location'] \n",
    "The coordinates are relative to the image on [0,1] (normalised COCO).\n",
    "\n",
    "Also works when there is no class name in the file structure, so that it can be used for inferance.  The 'Species' will then be 'unknown'\n",
    "'''\n",
    "import json\n",
    "from pathlib import Path\n",
    "import operator\n",
    "import pandas as pd\n",
    "import re\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import concurrent.futures\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "\n",
    "class DefaultConfig:\n",
    "    def __init__(self):\n",
    "        self.EXPERIMENT_NAME = 'MD_Last_Run'  #Must match the folder name under the Data folder\n",
    "        self.BEST_ONLY = True #If true, only the most probable prediction from each image is kept for training\n",
    "        self.CLASSES_FROM_DIR_NMS = True\n",
    "        self.CLASSES = ['mouse','robin','possum','stoat','cat','rat','thrush','kea','blackbird','wallaby','tomtit','cow',\n",
    "                        'sheep','human','rifleman','kiwi','rabbit','deer','weka','parakeet','ferret','hare','pukeko','harrier',\n",
    "                        'bellbird','hedgehog','chaffinch','dunnock','sealion','weasel','pipit','yellow_eyed_penguin','magpie',\n",
    "                        'myna','quail','greenfinch','yellowhammer','pig','kereru','tui','starling','sparrow','silvereye','fantail',\n",
    "                        'dog','moth','goat','pateke','banded_rail','oystercatcher','black_fronted_tern','paradise_duck','mallard',\n",
    "                        'morepork','goldfinch','chamois','redpoll','takahe','kaka','shore_plover','canada_goose','spurwing_plover',\n",
    "                        'tieke','white_faced_heron','lizard','shag','black_backed_gull','little_blue_penguin','brown_creeper',\n",
    "                        'black_billed_gull','crake','skylark','pheasant','skink','grey_warbler','swan','fernbird','banded_dotterel',\n",
    "                        'rosella','fiordland_crested_penguin','pied_stilt','mohua','long_tailed_cuckoo','kingfisher','nz_falcon',\n",
    "                        'grey_duck','spotted_dove','swallow'] + ['penguin', 'song thrush', 'bell', 'browncreeper', 'kakariki', \n",
    "                        'mice', 'tahr','waxeye', 'whio']\n",
    "        self.SOURCE_IMAGES_PTH = 'Z:\\\\alternative_footage\\\\CLEANED'\n",
    "        self.INDEPENDENT_TEST_ONLY = ['N01', 'BWS', 'EBF', 'EM1', 'ES1']\n",
    "        self.UPDATE_EXIF = True\n",
    "        self.UPDATE_META_DATA = True\n",
    "        \n",
    "        #Atributes that should not need changing below\n",
    "        self.LAST_MD_FOLDER_NM = 'MD_Last_Run'\n",
    "        self.LABELS_FROM_JSON_NM = 'all_labels.parquet' #Output label file.\n",
    "        self.EXIF_DATA_NM = 'last_exif_data.parquet' #Output exif data file\n",
    "        self.DATA_FOLDER_NM = 'Data'  #Directory for all data folders\n",
    "        self.EXPS_FOLDER_NM = 'Experiments'\n",
    "        self.INPUT_FOLDER_NM = 'Inputs'\n",
    "\n",
    "def get_config(settings_pth):\n",
    "    \"\"\"Gets an instance of the config class, then looks for the settings file, if it finds one evaluates specific strings to python expressions\"\"\"\n",
    "    evaluate_list = ['CLASSES', 'CLASSES_FROM_DIR_NMS', 'UPDATE_EXIF', 'UPDATE_META_DATA'] #any attributes with values that need evaluating from strings\n",
    "    cfg = DefaultConfig()\n",
    "    if settings_pth:\n",
    "        with open(settings_pth, 'r') as yaml_file:\n",
    "            yaml_data = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "        for key, value in yaml_data.items():\n",
    "            if hasattr(cfg, key):\n",
    "                if (key in evaluate_list) and (isinstance(value, str)):\n",
    "                    setattr(cfg, key, eval(value))\n",
    "                else:\n",
    "                    setattr(cfg, key, value)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_paths(cfg):\n",
    "    \"\"\"Sets up filepaths for this script using relative paths from the parent dir\"\"\"\n",
    "    project_dir = Path(r'E:\\Project')\n",
    "    data_folder = project_dir / cfg.DATA_FOLDER_NM \n",
    "    output_path = data_folder / cfg.EXPS_FOLDER_NM / cfg.EXPERIMENT_NAME / cfg.INPUT_FOLDER_NM / cfg.LABELS_FROM_JSON_NM\n",
    "    output_path_last_run = data_folder / cfg.EXPS_FOLDER_NM / cfg.LAST_MD_FOLDER_NM / cfg.LABELS_FROM_JSON_NM\n",
    "    exif_pth =data_folder / cfg.EXPS_FOLDER_NM / cfg.LAST_MD_FOLDER_NM  / cfg.EXIF_DATA_NM\n",
    "    json_file_folder = data_folder / cfg.EXPS_FOLDER_NM / cfg.EXPERIMENT_NAME / cfg.INPUT_FOLDER_NM\n",
    "    json_paths = [f for f in json_file_folder.glob('*.json')]\n",
    "    print(f'json files found in {json_file_folder}',json_paths)\n",
    "    dataset_pth = Path(cfg.SOURCE_IMAGES_PTH)\n",
    "    return output_path, exif_pth, output_path_last_run,   json_paths, dataset_pth, \n",
    "\n",
    "\n",
    "def iterate_json(jsons, best_only, data_pth_root):\n",
    "    \"\"\"Iterates through a single json file, extracts the useful values \n",
    "    and returns a list of lists, one list per image file\"\"\"\n",
    "    print(f'The root path of the image folder is {data_pth_root}')\n",
    "    def add_root_if_missing(fpath):\n",
    "        fpath = Path(fpath)\n",
    "        if not fpath.is_absolute():\n",
    "            fpath = Path(data_pth_root) / fpath\n",
    "        return str(fpath)\n",
    "    \n",
    "    for image in jsons:\n",
    "        detection_list = []\n",
    "        confidence_list = []\n",
    "        file_path = image.get('file')\n",
    "        detections = image.get('detections')\n",
    "        file_path = add_root_if_missing(file_path)    \n",
    "\n",
    "        if detections is not None:\n",
    "            for detect in detections:\n",
    "                predicted_class = detect['category']\n",
    "                confidence = detect['conf']\n",
    "                confidence_list.append(confidence)\n",
    "                left = detect['bbox'][0]  #x_min (normalised to image width)\n",
    "                top = detect['bbox'][1]  #y_min (normalised to image height)\n",
    "                width = detect['bbox'][2]  #box width  (normalised to image width)\n",
    "                height = detect['bbox'][3]  #box height (normalised to image height) \n",
    "                detection_list.append([file_path, predicted_class, confidence, left, top, width, height])\n",
    "            if detection_list:\n",
    "                if best_only:\n",
    "                    best_index, _ = max(enumerate(confidence_list), key=operator.itemgetter(1), default=(0, 0))\n",
    "                    detection_list = [detection_list[best_index]]\n",
    "            else: detection_list = [[file_path, -1, -1, 0, 0, 1, 1]] #This is for when detections is an empty list, it happens.\n",
    "        else:\n",
    "            detection_list = [[file_path, -1, -1, 0, 0, 1, 1]]\n",
    "        yield detection_list  # detection_list: A list of lists, with only one list if BEST_ONLY=True\n",
    "\n",
    "\n",
    "def get_class_list(grandparent_dir):\n",
    "    \"\"\"Takes the dataset root returns the foldernames of all the grandchildren, which should be the class names\"\"\"\n",
    "    parents = [folder for folder in grandparent_dir.iterdir() if folder.is_dir()]\n",
    "\n",
    "    master_list=[]\n",
    "    for parent in parents:\n",
    "        one_list = [folder.name for folder in parent.iterdir() if folder.is_dir()]\n",
    "        master_list = master_list + one_list\n",
    "    sorted_list = sorted(list(set(master_list)))\n",
    "    print(f'{len(sorted_list)} unique species found from folders: {sorted_list}')\n",
    "    return sorted_list\n",
    "\n",
    "\n",
    "def get_class_location(filepath, classes):\n",
    "    \"\"\"Parses the file path and returns any strings matching the class list, and 3 letter upper case strings\n",
    "    that are the unique identifier for each camera location\"\"\"\n",
    "    folder_names = re.split(r'[\\\\/]', filepath)\n",
    "    num_locations = 0\n",
    "    for name in folder_names:\n",
    "        if len(name) == 3 and any(char.isupper() for char in name):\n",
    "            num_locations +=1\n",
    "            location = name\n",
    "    if num_locations != 1:\n",
    "        location = 'unknown'\n",
    "\n",
    "    lowered_names = [f.lower() for f in folder_names] # In case a folder has accidental capitals\n",
    "    if classes:\n",
    "        class_name = next(iter(set(classes).intersection(set(lowered_names))), 'unknown')\n",
    "    else:\n",
    "        class_name = lowered_names[folder_names.index(location) + 1]\n",
    "    return class_name, location\n",
    "\n",
    "\n",
    "def load_json(json_path):\n",
    "    \"\"\"Opens a single json file and loads into an array of dictionaries, returns that array\n",
    "    each dict has the keys 'file', 'max_detection_conf', 'detections' \"\"\"\n",
    "    with open(json_path) as json_file:\n",
    "        json_dict = json.load(json_file)\n",
    "        json_array = json_dict['images']\n",
    "        if __name__ == '__main__':\n",
    "            print(\"Number of images in json array\", len(json_array))\n",
    "    return json_array\n",
    "\n",
    "\n",
    "def process_json_array(json_path, classes, image_folder, best_only=True):\n",
    "    \"\"\"Iterates through each item of image data, extract the bits of interest, returns a dataframe\n",
    "    #each image has potentially several crops in a list if BEST_ONLY=False\"\"\"\n",
    "    OUT_COLUMNS = ['File_Path', 'Mega_Class', 'Confidence', 'x_min', 'y_min', 'Width', 'Height', 'Species', 'Location']\n",
    "    j_array = load_json(json_path)\n",
    "    data_rows = []\n",
    "    if __name__ == '__main__': \n",
    "        pbar = tqdm(total=len(j_array), desc=\"Processing JSON file items\")\n",
    "    for detects in iterate_json(j_array, best_only, image_folder):\n",
    "        if detects:\n",
    "            for one_thing in detects:\n",
    "                fpath = one_thing[0]\n",
    "                observed_class, cam_location = get_class_location(fpath, classes)\n",
    "                new_row = one_thing + [observed_class, cam_location]\n",
    "                data_rows.append(new_row)\n",
    "        if __name__ == '__main__':\n",
    "            pbar.update(1)\n",
    "    if __name__ == '__main__':\n",
    "        pbar.close()\n",
    "    df = pd.DataFrame(data_rows, columns=OUT_COLUMNS)\n",
    "    return df\n",
    "\n",
    "def process_all_jsons(json_path_list, classes, image_folder=None, best_only=True):\n",
    "    \"\"\"Iterates throguh a list of json files and appends the result from each to a single dataframe which is returned\"\"\"\n",
    "    OUT_COLUMNS = ['File_Path', 'Mega_Class', 'Confidence', 'x_min', 'y_min', 'Width', 'Height', 'Species', 'Location']\n",
    "    dataframe = pd.DataFrame(columns=OUT_COLUMNS)\n",
    "    for json_path in json_path_list: \n",
    "        df = process_json_array(json_path, classes, image_folder, best_only)\n",
    "        print(f'The dataframe length after processing {json_path}: {len(df)}')\n",
    "        unknown_rows = df[df['Species'] == 'unknown']\n",
    "        if __name__ == '__main__':\n",
    "            print('Unknown species:')\n",
    "            print(unknown_rows.head(3))\n",
    "        dataframe = pd.concat([dataframe,df])\n",
    "\n",
    "    if __name__ == '__main__': \n",
    "        unknown_class = dataframe['Species'].value_counts().get('unknown', 0)\n",
    "        unknown_place = dataframe['Location'].value_counts().get('unknown', 0)\n",
    "        species = dataframe['Species'].nunique() - (unknown_class!=0)\n",
    "        places = dataframe['Location'].nunique() - (unknown_place!=0)\n",
    "        print(dataframe.head())\n",
    "        print(f'{species} Unique species found')\n",
    "        print(f'{places} Unique dataset location folders found')\n",
    "        print(f'{unknown_class} Entries that had an unknown class')\n",
    "        print(f'{unknown_place} Entries that had an unknown location')\n",
    "        print(f'{len(dataframe)} rows from the MegaDetector Runs')\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def remove_missing_dirs(root, df):\n",
    "    \"\"\"This compares a list of location + class, from the source directory, with the same from\n",
    "    the final dataframe (generated by the MegaDetector), and removes any rows where those folders \n",
    "    are missing.  Necessary if folders have been removed since the MegaDetector was run\n",
    "    Saves time for the next step, which looks at individual files.\"\"\"\n",
    "    loc_spec_lst = df.groupby(['Location', 'Species']).size().reset_index()[['Location', 'Species']].agg(tuple, axis=1).tolist()\n",
    "    print('Checking by folder name if any instances from the MegaDetector have been removed')\n",
    "    folders_to_keep = []\n",
    "    for item in tqdm(loc_spec_lst):\n",
    "        location = item[0]\n",
    "        species = item[1]\n",
    "        dir_path = root / location / species\n",
    "        if dir_path.exists() and dir_path.is_dir():\n",
    "            folders_to_keep.append(item)\n",
    "\n",
    "    tuples_df = pd.DataFrame(folders_to_keep, columns=['Location', 'Species'])\n",
    "    filtered_df = df.merge(tuples_df, on=['Location', 'Species'], how='inner')\n",
    "    print(f'{len(df) - len(filtered_df)} rows were removed from the dataframe')\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def remove_old_filepaths(root_dir, df):\n",
    "    \"\"\"Remove from the dataframe any old filepaths that have been deleted or removed \n",
    "       from the source directory\n",
    "    Args:\n",
    "        root_dir (path object): path to the root directory to be searched\n",
    "        df (dataframe): dataframe with filepaths as path objects\n",
    "    Returns:\n",
    "        dataframe: dataframe with the lines matching missing filepaths removed\n",
    "    \"\"\"\n",
    "    tqdm.pandas()\n",
    "    def string_to_path(x):\n",
    "        return Path(x)\n",
    "    df['File_Path_Objects'] = df['File_Path'].progress_apply(string_to_path)\n",
    "    print('Getting a unique list from the new file path object column')\n",
    "    df_fnames = set(df['File_Path_Objects'].unique().tolist())\n",
    "    subdirs = [entry for entry in root_dir.iterdir() if entry.is_dir()]\n",
    "\n",
    "    def search_for_jpgs(folder):\n",
    "        jpgs = {path for path in folder.rglob('*.[jJ][pP][gG]')}\n",
    "        return jpgs\n",
    "    \n",
    "    print('Searching the MD output to make a set of all the existing jpg or JPG files')\n",
    "    found_filenames = set() \n",
    "    overall_progress = tqdm(total=len(subdirs), desc=\"Folders processed\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "        for result in executor.map(search_for_jpgs, subdirs):\n",
    "            found_filenames.update(result)\n",
    "            overall_progress.update(1)\n",
    "    overall_progress.close()\n",
    "\n",
    "    missing_files = list(df_fnames - found_filenames)\n",
    "    print(f'Removing {len(missing_files)} files from the DataFrame as they cannot be found')\n",
    "    print(missing_files[:10])\n",
    "    df = df[~df['File_Path_Objects'].isin(missing_files)]\n",
    "    del df['File_Path_Objects']\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def extract_exif_data(image_path):   #https://exiv2.org/tags.html    #306=DateTime, #36867=DateTimeOriginal 270=ImageDescription\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            exif_data = img.getexif()\n",
    "            if not exif_data:\n",
    "                return {'File_Path': str(image_path), 'Date_Time': 'exif_not_found', 'Description': 'exif_not_found'}\n",
    "            tag_list = [306, 36867, 270]\n",
    "            dt, description = 'dt_not_found', 'description_not_found'\n",
    "\n",
    "            for tag in tag_list:\n",
    "                try:\n",
    "                    value = exif_data.get(tag)\n",
    "                    if value is not None and not value.isspace():\n",
    "                        if tag == 306 or tag == 36867:\n",
    "                            dt = value\n",
    "                        elif tag == 270:\n",
    "                            description = value\n",
    "                except KeyError:\n",
    "                    continue\n",
    "            return {'File_Path': str(image_path), 'Date_Time': dt, 'Description': description}\n",
    "    except (FileNotFoundError, OSError):\n",
    "        return {'File_Path': str(image_path), 'Date_Time': 'file_not_valid', 'Description': 'file_not_valid'}\n",
    "\n",
    "\n",
    "def get_last_exif_data(file_path):\n",
    "    \"\"\"Looks for an existing parquet file, and returns a dataframe.  If no file found, \n",
    "    a dataframe is returned with the right column names, no rows\"\"\"\n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "    except FileNotFoundError:\n",
    "        column_names = ['File_Path', 'Date_Time', 'Description']\n",
    "        df = pd.DataFrame(columns=column_names).astype(str)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Exp_27'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "settings_pth=r'E:\\Project\\Settings\\Exp_27_Run_01.yaml'\n",
    "cfg = get_config(settings_pth)\n",
    "#cfg.UPDATE_META_DATA\n",
    "cfg.EXPERIMENT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json files found in E:\\Project\\Data\\Experiments\\Exp_27\\Inputs [WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output.json'), WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output_July_23.json'), WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output_22_sept.json'), WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output_25_Nov.json')]\n",
      "98 unique species found from folders: ['banded_dotterel', 'banded_rail', 'bellbird', 'black_backed_gull', 'black_billed_gull', 'black_fronted_tern', 'blackbird', 'brown_creeper', 'canada_goose', 'cat', 'chaffinch', 'chamois', 'chicken', 'cow', 'crake', 'deer', 'dog', 'dunnock', 'fantail', 'fernbird', 'ferret', 'fiordland_crested_penguin', 'fluttering_shearwater', 'goat', 'goldfinch', 'greenfinch', 'grey_duck', 'grey_faced_petrol', 'grey_warbler', 'hare', 'harrier', 'hedgehog', 'horse', 'human', 'kaka', 'kea', 'kereru', 'kingfisher', 'kiwi', 'little_blue_penguin', 'lizard', 'long_tailed_cuckoo', 'magpie', 'mallard', 'mohua', 'morepork', 'moth', 'mouse', 'myna', 'nz_falcon', 'oystercatcher', 'paradise_duck', 'parakeet', 'pateke', 'pheasant', 'pig', 'pipit', 'plover', 'possum', 'pukeko', 'quail_brown', 'quail_california', 'rabbit', 'rat', 'redpoll', 'rifleman', 'robin', 'rosella', 'sealion', 'shag', 'sheep', 'shore_plover', 'silvereye', 'skink', 'sparrow', 'spotted_dove', 'spurwing_plover', 'starling', 'stilt', 'stoat', 'swallow', 'swan', 'tahr', 'takahe', 'thrush', 'tieke', 'tomtit', 'tui', 'wallaby', 'weasel', 'weka', 'welcome_swallow', 'white_faced_heron', 'white_tailed_deer', 'whitehead', 'wrybill', 'yellow_eyed_penguin', 'yellowhammer']\n",
      "Number of images in json array 657292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   1%|          | 5240/657292 [00:00<00:12, 52072.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 657292/657292 [00:12<00:00, 51080.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output.json: 657292\n",
      "Unknown species:\n",
      "Empty DataFrame\n",
      "Columns: [File_Path, Mega_Class, Confidence, x_min, y_min, Width, Height, Species, Location]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25972\\3433530636.py:198: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe,df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in json array 2320443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   0%|          | 7430/2320443 [00:00<00:31, 73998.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 2320443/2320443 [00:31<00:00, 73821.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output_July_23.json: 2320443\n",
      "Unknown species:\n",
      "                                               File_Path Mega_Class  \\\n",
      "27092  Z:\\alternative_footage\\CLEANED\\BWS\\quail\\08097...          1   \n",
      "27093  Z:\\alternative_footage\\CLEANED\\BWS\\quail\\3f2cc...          1   \n",
      "27094  Z:\\alternative_footage\\CLEANED\\BWS\\quail\\3fce1...          1   \n",
      "\n",
      "       Confidence    x_min   y_min    Width   Height  Species Location  \n",
      "27092       0.837  0.07012  0.3852  0.01841  0.05886  unknown      BWS  \n",
      "27093       0.761  0.31750  0.8009  0.10080  0.16860  unknown      BWS  \n",
      "27094       0.880  0.18240  0.3988  0.09040  0.09722  unknown      BWS  \n",
      "Number of images in json array 177703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   6%|▌         | 10611/177703 [00:00<00:03, 53051.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 177703/177703 [00:03<00:00, 51216.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output_22_sept.json: 177703\n",
      "Unknown species:\n",
      "                                               File_Path Mega_Class  \\\n",
      "93962  Z:\\alternative_footage\\CLEANED\\LCA\\rodent\\8014...          1   \n",
      "93963  Z:\\alternative_footage\\CLEANED\\LCA\\rodent\\9d38...          1   \n",
      "93964  Z:\\alternative_footage\\CLEANED\\LCA\\rodent\\c6d1...          1   \n",
      "\n",
      "       Confidence   x_min   y_min    Width   Height  Species Location  \n",
      "93962     0.10400  0.6182  0.6335  0.08731  0.05964  unknown      LCA  \n",
      "93963     0.08630  0.6942  0.4587  0.01927  0.03611  unknown      LCA  \n",
      "93964     0.00862  0.8137  0.8464  0.03615  0.11230  unknown      LCA  \n",
      "Number of images in json array 550850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   2%|▏         | 10338/550850 [00:00<00:10, 51017.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 550850/550850 [00:10<00:00, 51016.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output_25_Nov.json: 550850\n",
      "Unknown species:\n",
      "                                                File_Path Mega_Class  \\\n",
      "332181  Z:\\alternative_footage\\CLEANED\\PS3\\kakariki\\20...          1   \n",
      "332182  Z:\\alternative_footage\\CLEANED\\PS3\\kakariki\\5f...          1   \n",
      "332183  Z:\\alternative_footage\\CLEANED\\PS3\\kakariki\\69...          1   \n",
      "\n",
      "        Confidence   x_min   y_min    Width  Height  Species Location  \n",
      "332181       0.581  0.7920  0.6604  0.15280  0.1402  unknown      PS3  \n",
      "332182       0.922  0.2984  0.4750  0.09006  0.1503  unknown      PS3  \n",
      "332183       0.552  0.3020  0.4957  0.11330  0.1233  unknown      PS3  \n",
      "                                           File_Path Mega_Class  Confidence  \\\n",
      "0  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\aded7...          1       0.721   \n",
      "1  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\66ea1...          1       0.891   \n",
      "2  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\258cd...          1       0.950   \n",
      "3  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\56c32...          1       0.809   \n",
      "4  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\02504...          1       0.925   \n",
      "\n",
      "    x_min   y_min    Width  Height Species Location  \n",
      "0  0.1986  0.4425  0.04990  0.1005   robin      PS1  \n",
      "1  0.3172  0.3475  0.08531  0.1304   robin      PS1  \n",
      "2  0.5655  0.4527  0.07366  0.2474   robin      PS1  \n",
      "3  0.2328  0.5502  0.04871  0.1173   robin      PS1  \n",
      "4  0.6955  0.4987  0.04230  0.1279   robin      PS1  \n",
      "98 Unique species found\n",
      "63 Unique dataset location folders found\n",
      "150806 Entries that had an unknown class\n",
      "0 Entries that had an unknown location\n",
      "3706288 rows from the MegaDetector Runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Path</th>\n",
       "      <th>Mega_Class</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Species</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\aded7...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.04990</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\66ea1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.3172</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.08531</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\258cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>0.07366</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\56c32...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.5502</td>\n",
       "      <td>0.04871</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\02504...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.04230</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_Path Mega_Class  Confidence  \\\n",
       "0  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\aded7...          1       0.721   \n",
       "1  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\66ea1...          1       0.891   \n",
       "2  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\258cd...          1       0.950   \n",
       "3  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\56c32...          1       0.809   \n",
       "4  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\02504...          1       0.925   \n",
       "\n",
       "    x_min   y_min    Width  Height Species Location  \n",
       "0  0.1986  0.4425  0.04990  0.1005   robin      PS1  \n",
       "1  0.3172  0.3475  0.08531  0.1304   robin      PS1  \n",
       "2  0.5655  0.4527  0.07366  0.2474   robin      PS1  \n",
       "3  0.2328  0.5502  0.04871  0.1173   robin      PS1  \n",
       "4  0.6955  0.4987  0.04230  0.1279   robin      PS1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path, exif_path, last_run_path, json_paths, data_root = get_paths(cfg)\n",
    "if cfg.CLASSES_FROM_DIR_NMS:\n",
    "    class_list = get_class_list(data_root)\n",
    "else: class_list = cfg.CLASSES\n",
    "all_processed = process_all_jsons(json_paths, class_list,  data_root, cfg.BEST_ONLY)\n",
    "all_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking by folder name if any instances from the MegaDetector have been removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928/928 [00:02<00:00, 427.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326074 rows were removed from the dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3706288/3706288 [00:23<00:00, 156706.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a unique list from the new file path object column\n",
      "Searching the MD output to make a set of all the existing jpg or JPG files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folders processed: 100%|██████████| 64/64 [11:40<00:00, 10.95s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 520048 files from the DataFrame as they cannot be found\n",
      "[WindowsPath('Z:/alternative_footage/CLEANED/Z2O/mice/51d1a894-6d2d-4a8d-aef2-ab608d2c0f59.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/PS1/robin/CAM8505/01df2378-5f46-4231-8e57-ae87e1e77a74.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/PS1/robin/CAM6314/3adbbb9e-6b65-49ca-bd8c-fdb6264f0352.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/kiwi/41de21cf-33f4-4a3e-b8d6-886ead4f7ea4.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/robin/e2d8de4b-95f4-40a3-80a2-791ea5dfd8dd.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/robin/a12bf64f-ced3-464c-8594-6a4e8b18ad56.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/mice/f1555bc2-5c24-4828-a24e-53dcc4b232bf.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/mice/6af81620-f7dd-4aed-ad9a-89116f69e2c8.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/waxeye/632af883-c5ec-4639-8f04-57cf5dfbfd3e.JPG'), WindowsPath('Z:/alternative_footage/CLEANED/Z2O/mice/95963266-0d4a-4268-a553-2d04f2be7a7d.JPG')]\n",
      "There are 157461 from the MD output json files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:20<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 137413 rats in the dataset folders (including the independent test set folders)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 13.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2091 rats in the independent test folders, so 135322 for training\n",
      "There are 3185487 rows in the final dataframe to be written to parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_json = remove_missing_dirs(data_root, all_processed)\n",
    "df_from_json = remove_old_filepaths(data_root, all_processed)\n",
    "\n",
    "#Print some stats about rats, just as a sanity check\n",
    "num_rats = (df_from_json['Species'] == 'rat').sum()\n",
    "print(f'There are {num_rats} from the MD output json files')\n",
    "rat_folders = [grandchild for grandchild in data_root.glob('*/*') if grandchild.is_dir() and grandchild.name == 'rat']\n",
    "rat_count = 0\n",
    "for rat_folder in tqdm(rat_folders):\n",
    "    rat_count += sum(1 for _ in rat_folder.glob('*.[jJ][pP][gG]'))\n",
    "print(f'There are {rat_count} rats in the dataset folders (including the independent test set folders)')\n",
    "independent_rats = [rat_folder for rat_folder in rat_folders if rat_folder.parent.name in cfg.INDEPENDENT_TEST_ONLY]\n",
    "independent_rat_count = 0\n",
    "for rat_folder in tqdm(independent_rats):\n",
    "    independent_rat_count += sum(1 for _ in rat_folder.glob('*.[jJ][pP][gG]'))\n",
    "print(f'There are {independent_rat_count} rats in the independent test folders, so {rat_count - independent_rat_count} for training')\n",
    "print(f'There are {len(df_from_json)} rows in the final dataframe to be written to parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:\\Project\\Data\\Experiments\\MD_Last_Run\\last_exif_data.parquet\n"
     ]
    }
   ],
   "source": [
    "print(exif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Path</th>\n",
       "      <th>Mega_Class</th>\n",
       "      <th>Confidence</th>\n",
       "      <th>x_min</th>\n",
       "      <th>y_min</th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Species</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\aded7...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.721</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>0.4425</td>\n",
       "      <td>0.04990</td>\n",
       "      <td>0.1005</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\66ea1...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.3172</td>\n",
       "      <td>0.3475</td>\n",
       "      <td>0.08531</td>\n",
       "      <td>0.1304</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\258cd...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.950</td>\n",
       "      <td>0.5655</td>\n",
       "      <td>0.4527</td>\n",
       "      <td>0.07366</td>\n",
       "      <td>0.2474</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\56c32...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.2328</td>\n",
       "      <td>0.5502</td>\n",
       "      <td>0.04871</td>\n",
       "      <td>0.1173</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\PS1\\robin\\02504...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.6955</td>\n",
       "      <td>0.4987</td>\n",
       "      <td>0.04230</td>\n",
       "      <td>0.1279</td>\n",
       "      <td>robin</td>\n",
       "      <td>PS1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_Path Mega_Class  Confidence  \\\n",
       "0  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\aded7...          1       0.721   \n",
       "1  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\66ea1...          1       0.891   \n",
       "2  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\258cd...          1       0.950   \n",
       "3  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\56c32...          1       0.809   \n",
       "4  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\02504...          1       0.925   \n",
       "\n",
       "    x_min   y_min    Width  Height Species Location  \n",
       "0  0.1986  0.4425  0.04990  0.1005   robin      PS1  \n",
       "1  0.3172  0.3475  0.08531  0.1304   robin      PS1  \n",
       "2  0.5655  0.4527  0.07366  0.2474   robin      PS1  \n",
       "3  0.2328  0.5502  0.04871  0.1173   robin      PS1  \n",
       "4  0.6955  0.4987  0.04230  0.1279   robin      PS1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File_Path      object\n",
       "Mega_Class     object\n",
       "Confidence    float64\n",
       "x_min         float64\n",
       "y_min         float64\n",
       "Width         float64\n",
       "Height        float64\n",
       "Species        object\n",
       "Location       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_json.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '2', '3', -1], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_json.Mega_Class.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File_Path</th>\n",
       "      <th>Date_Time</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\EFD\\stoat\\5B459...</td>\n",
       "      <td>2020:09:05 11:30:04</td>\n",
       "      <td>EFD__DCAME03__stoat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\EFH\\mouse\\folde...</td>\n",
       "      <td>2019:10:26 04:22:41</td>\n",
       "      <td>EFH__HCAMA12__mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\EFH\\mouse\\folde...</td>\n",
       "      <td>2020:02:24 22:54:17</td>\n",
       "      <td>EFH__HCAMB03__mouse</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\EFH\\rat\\765B79E...</td>\n",
       "      <td>2018:01:06 08:58:02</td>\n",
       "      <td>EFH__HCAMB05__rat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Z:\\alternative_footage\\CLEANED\\EFH\\robin\\4E850...</td>\n",
       "      <td>2019:06:26 14:39:22</td>\n",
       "      <td>EFH__HCAMF04__robin</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           File_Path            Date_Time  \\\n",
       "0  Z:\\alternative_footage\\CLEANED\\EFD\\stoat\\5B459...  2020:09:05 11:30:04   \n",
       "1  Z:\\alternative_footage\\CLEANED\\EFH\\mouse\\folde...  2019:10:26 04:22:41   \n",
       "2  Z:\\alternative_footage\\CLEANED\\EFH\\mouse\\folde...  2020:02:24 22:54:17   \n",
       "3  Z:\\alternative_footage\\CLEANED\\EFH\\rat\\765B79E...  2018:01:06 08:58:02   \n",
       "4  Z:\\alternative_footage\\CLEANED\\EFH\\robin\\4E850...  2019:06:26 14:39:22   \n",
       "\n",
       "           Description  \n",
       "0  EFD__DCAME03__stoat  \n",
       "1  EFH__HCAMA12__mouse  \n",
       "2  EFH__HCAMB03__mouse  \n",
       "3    EFH__HCAMB05__rat  \n",
       "4  EFH__HCAMF04__robin  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths = df_from_json['File_Path'].tolist()\n",
    "exif_df = get_last_exif_data(exif_path)\n",
    "exif_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json files found in E:\\Project\\Data\\Experiments\\Exp_27\\Inputs [WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output.json'), WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output_July_23.json'), WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output_22_sept.json'), WindowsPath('E:/Project/Data/Experiments/Exp_27/Inputs/MD_output_25_Nov.json')]\n",
      "98 unique species found from folders: ['banded_dotterel', 'banded_rail', 'bellbird', 'black_backed_gull', 'black_billed_gull', 'black_fronted_tern', 'blackbird', 'brown_creeper', 'canada_goose', 'cat', 'chaffinch', 'chamois', 'chicken', 'cow', 'crake', 'deer', 'dog', 'dunnock', 'fantail', 'fernbird', 'ferret', 'fiordland_crested_penguin', 'fluttering_shearwater', 'goat', 'goldfinch', 'greenfinch', 'grey_duck', 'grey_faced_petrol', 'grey_warbler', 'hare', 'harrier', 'hedgehog', 'horse', 'human', 'kaka', 'kea', 'kereru', 'kingfisher', 'kiwi', 'little_blue_penguin', 'lizard', 'long_tailed_cuckoo', 'magpie', 'mallard', 'mohua', 'morepork', 'moth', 'mouse', 'myna', 'nz_falcon', 'oystercatcher', 'paradise_duck', 'parakeet', 'pateke', 'pheasant', 'pig', 'pipit', 'plover', 'possum', 'pukeko', 'quail_brown', 'quail_california', 'rabbit', 'rat', 'redpoll', 'rifleman', 'robin', 'rosella', 'sealion', 'shag', 'sheep', 'shore_plover', 'silvereye', 'skink', 'sparrow', 'spotted_dove', 'spurwing_plover', 'starling', 'stilt', 'stoat', 'swallow', 'swan', 'tahr', 'takahe', 'thrush', 'tieke', 'tomtit', 'tui', 'wallaby', 'weasel', 'weka', 'welcome_swallow', 'white_faced_heron', 'white_tailed_deer', 'whitehead', 'wrybill', 'yellow_eyed_penguin', 'yellowhammer']\n",
      "Number of images in json array 657292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   1%|          | 5397/657292 [00:00<00:12, 51510.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 657292/657292 [00:13<00:00, 49402.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output.json: 657292\n",
      "Unknown species:\n",
      "Empty DataFrame\n",
      "Columns: [File_Path, Mega_Class, Confidence, x_min, y_min, Width, Height, Species, Location]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6160\\2707220670.py:198: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe,df])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in json array 2320443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   0%|          | 7505/2320443 [00:00<00:30, 74620.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 2320443/2320443 [00:31<00:00, 74124.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output_July_23.json: 2320443\n",
      "Unknown species:\n",
      "                                               File_Path Mega_Class  \\\n",
      "27092  Z:\\alternative_footage\\CLEANED\\BWS\\quail\\08097...          1   \n",
      "27093  Z:\\alternative_footage\\CLEANED\\BWS\\quail\\3f2cc...          1   \n",
      "27094  Z:\\alternative_footage\\CLEANED\\BWS\\quail\\3fce1...          1   \n",
      "\n",
      "       Confidence    x_min   y_min    Width   Height  Species Location  \n",
      "27092       0.837  0.07012  0.3852  0.01841  0.05886  unknown      BWS  \n",
      "27093       0.761  0.31750  0.8009  0.10080  0.16860  unknown      BWS  \n",
      "27094       0.880  0.18240  0.3988  0.09040  0.09722  unknown      BWS  \n",
      "Number of images in json array 177703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   6%|▌         | 9870/177703 [00:00<00:03, 49048.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 177703/177703 [00:03<00:00, 45199.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output_22_sept.json: 177703\n",
      "Unknown species:\n",
      "                                               File_Path Mega_Class  \\\n",
      "93962  Z:\\alternative_footage\\CLEANED\\LCA\\rodent\\8014...          1   \n",
      "93963  Z:\\alternative_footage\\CLEANED\\LCA\\rodent\\9d38...          1   \n",
      "93964  Z:\\alternative_footage\\CLEANED\\LCA\\rodent\\c6d1...          1   \n",
      "\n",
      "       Confidence   x_min   y_min    Width   Height  Species Location  \n",
      "93962     0.10400  0.6182  0.6335  0.08731  0.05964  unknown      LCA  \n",
      "93963     0.08630  0.6942  0.4587  0.01927  0.03611  unknown      LCA  \n",
      "93964     0.00862  0.8137  0.8464  0.03615  0.11230  unknown      LCA  \n",
      "Number of images in json array 550850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items:   2%|▏         | 10232/550850 [00:00<00:10, 50560.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The root path of the image folder is Z:\\alternative_footage\\CLEANED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing JSON file items: 100%|██████████| 550850/550850 [00:11<00:00, 45923.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe length after processing E:\\Project\\Data\\Experiments\\Exp_27\\Inputs\\MD_output_25_Nov.json: 550850\n",
      "Unknown species:\n",
      "                                                File_Path Mega_Class  \\\n",
      "332181  Z:\\alternative_footage\\CLEANED\\PS3\\kakariki\\20...          1   \n",
      "332182  Z:\\alternative_footage\\CLEANED\\PS3\\kakariki\\5f...          1   \n",
      "332183  Z:\\alternative_footage\\CLEANED\\PS3\\kakariki\\69...          1   \n",
      "\n",
      "        Confidence   x_min   y_min    Width  Height  Species Location  \n",
      "332181       0.581  0.7920  0.6604  0.15280  0.1402  unknown      PS3  \n",
      "332182       0.922  0.2984  0.4750  0.09006  0.1503  unknown      PS3  \n",
      "332183       0.552  0.3020  0.4957  0.11330  0.1233  unknown      PS3  \n",
      "                                           File_Path Mega_Class  Confidence  \\\n",
      "0  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\aded7...          1       0.721   \n",
      "1  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\66ea1...          1       0.891   \n",
      "2  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\258cd...          1       0.950   \n",
      "3  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\56c32...          1       0.809   \n",
      "4  Z:\\alternative_footage\\CLEANED\\PS1\\robin\\02504...          1       0.925   \n",
      "\n",
      "    x_min   y_min    Width  Height Species Location  \n",
      "0  0.1986  0.4425  0.04990  0.1005   robin      PS1  \n",
      "1  0.3172  0.3475  0.08531  0.1304   robin      PS1  \n",
      "2  0.5655  0.4527  0.07366  0.2474   robin      PS1  \n",
      "3  0.2328  0.5502  0.04871  0.1173   robin      PS1  \n",
      "4  0.6955  0.4987  0.04230  0.1279   robin      PS1  \n",
      "98 Unique species found\n",
      "63 Unique dataset location folders found\n",
      "150806 Entries that had an unknown class\n",
      "0 Entries that had an unknown location\n",
      "3706288 rows from the MegaDetector Runs\n",
      "Checking by folder name if any instances from the MegaDetector have been removed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 928/928 [00:02<00:00, 420.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326074 rows were removed from the dataframe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3706288/3706288 [00:22<00:00, 161750.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting a unique list from the new file path object column\n",
      "Searching the MD output to make a set of all the existing jpg or JPG files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folders processed:   5%|▍         | 3/64 [00:33<11:42, 11.52s/it]"
     ]
    }
   ],
   "source": [
    "print(f'There are {len(exif_df)} rows found in the previous exif data file')\n",
    "if cfg.UPDATE_EXIF:\n",
    "    exif_df = exif_df[exif_df['File_Path'].isin(file_paths)]\n",
    "    print(f'After filtering based on File_Path, there are {len(exif_df)} rows left')\n",
    "    exif_df = exif_df[exif_df['File_Path'] != 'file_not_valid']\n",
    "    print(f'After filtering based on Date_Time, there are {len(exif_df)} rows left')\n",
    "    exif_df = exif_df[exif_df['Date_Time'] != 'dt_not_found']\n",
    "    print(f'After filtering based on Date_Time (again), there are {len(exif_df)} rows left')\n",
    "    exif_df = exif_df[exif_df['Description'] != 'description_not_found']\n",
    "    print(f'After filtering based on Description, there are {len(exif_df)} rows of exif data left left')\n",
    "    already_have_exif = exif_df['File_Path'].to_list()\n",
    "    file_paths = list(set(file_paths).difference(set(already_have_exif)))\n",
    "    print(f'Extracting EXIF data from files without complete EXIF data')\n",
    "    new_exif_data = Parallel(n_jobs=8, prefer='threads')(delayed(extract_exif_data)(fp) for fp in tqdm(file_paths))\n",
    "    new_exif_df = pd.DataFrame(new_exif_data)\n",
    "    if len(new_exif_data)>=1:\n",
    "        new_exif_df = new_exif_df[new_exif_df['Date_Time'] != 'file_not_valid']\n",
    "        exif_df = pd.concat([exif_df, new_exif_df], ignore_index=True)\n",
    "\n",
    "exif_df['File_Path'] = exif_df['File_Path'].astype(str) #can probably remove this\n",
    "data_out = df_from_json.merge(exif_df, on='File_Path', how='inner', suffixes=('', '')) #merges vertically, removing any rows not present in both\n",
    "print('Final dataframe from interpreting jsons and exif data')\n",
    "print(data_out.head())\n",
    "exif_df.to_parquet(exif_path) # Updates the 'MD_Last_Run' folder\n",
    "data_out.to_parquet(output_path) # Updates the labels file before cleaning step.\n",
    "data_out.to_parquet(last_run_path) # Updates the 'MD_Last_Run' folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
